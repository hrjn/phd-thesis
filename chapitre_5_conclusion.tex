\chapter{Conclusion}

Dans le cadre d'un rejet de polluant dans l'atmosphère, que celui-ci soit d'origine accidentelle ou malveillante, des outils de modélisation numérique sont utilisés pour en quantifier l'impact. La qualité des résultats issus de ces modèles dépend des variables d'entrée qui lui sont fournies, notamment concernant les paramètres de la source à l'origine du rejet. Or dans la plupart des cas, l'information sur cette source n'est pas immédiatement disponible, et nécessite donc d'être estimée à l'aide de méthodes appropriées. \\

Plusieurs solutions pour résoudre ce problème sont abordées dans la littérature scientifique, utilisant des méthodologies variées telles que l'optimisation convexe, les algorithmes génétiques, ou l'inférence bayésienne pour retrouver les caractéristique d'une source à partir des mesures fournies par un réseau de capteurs. L'approche bayésienne aborde la question de l'estimation du terme source sous un aspect probabiliste, en cherchant à construire la densité de probabilité a posteriori des paramètres à retrouver. Un tel choix est justifié par l'incertitude qui entoure les observations fournies par les détecteurs, mais est rendu difficile à cause de l'aspect non-linéaire des phénomènes physiques qui rentrent en jeu dans l'atmosphère. Il devient alors nécessaire d'avoir recours à des méthodes de type Monte-Carlo, basées sur la simulation stochastique, et permettant d'approximer numériquement la loi a posteriori recherchée. \\

Lors du processus d'estimation du terme source, l'objectif mathématique est de minimiser une fonction-coût entre les observations fournies par les capteurs, et les concentrations simulées par un modèle de dispersion atmosphérique sur un ensemble de sources potentielles. Dans un contexte déterministe, cette fonction-coût est optimisée en explorant systématiquement tous les points d'un maillage sur l'espace et le temps, pouvant mener à des problèmes de très grande dimension selon la densité du maillage choisi. 

L'approche stochastique privilégie un parcours itératif de l'espace des paramètres, soit en suivant les états successifs d'une chaîne de Markov guidée par une procédure d'acceptation-rejet, soit en échantillonnant une population de particules qui sont ensuite pondérées suivant leur cohérence vis-à-vis des observations. Dans les deux cas, il est nécessaire d'exécuter un modèle de dispersion pour évaluer l'état ou la particule candidat(e), et un tel modèle peut se révéler coûteux en temps et en charge de calcul en fonction de son niveau de complexité. \\

Dans cette thèse, nous avons donc cherché à construire une chaîne de calcul  basée sur une méthode bayésienne stochastique permettant d'estimer à la fois la position de la source, ainsi que son profil temporel d'émission, autrement dit l'historique des quantités rejetées au fil du temps.

\section{Résultats}

Le premier chapitre de ce manuscrit permet de définir le contexte dans lequel s'inscrit le travail de thèse, et de faire une synthèse des méthodes existantes, avec leurs avantages et leurs inconvénients respectifs.\\


Dans le chapitre 2, nous rappelons les principes généraux qui régissent l'inférence bayésienne, avant de souligner l'intérêt des méthodes de Monte-Carlo pour la résolution des problèmes où la formulation analytique de la loi a posteriori est impossible. Parmi ces méthodes, nous évoquons deux approches différentes: les algorithmes de type Markov Chain Monte-Carlo (MCMC) et l'échantillonnage d'importance (IS). Nous étudions ensuite plus en détail l'introduction d'une phase d'adaptation dans les méthodes IS, permettant ainsi à la loi de proposition d'évoluer au fil des itérations en fonction des particules et des poids d'importance précédemment calculés. Cet aspect adaptatif est étendu aux poids d'importance, dont la totalité est tout d'abord recyclée, puis réutilisée dans l'implémentation de l'algorithme Adaptive Multiple Importance Sampling (AMIS), qui constitue une amélioration efficace de l'IS standard. \\

Le chapitre 3 illustre l'application de l'algorithme AMIS au contexte de l'estimation de source, en utilisant les résultats de la campagne de mesures expérimentales FFT07. Dans la formulation mathématique du problème, nous introduisons un a priori gaussien sur les éléments du profil temporel de rejet, ce qui a permis d'en obtenir une estimation analytique, et de limiter les calculs de simulation stochastique à la partie "localisation" de la source. Dans l'implémentation de cette dernière, nous présentons un modèle de loi de proposition basé sur une mixture de gaussiennes. Le principal avantage de ce type de distribution réside dans la phase de mise à jour des paramètres, dont les équations sont directement données par l'algorithme Expectation-Maximization (EM). Un tel modèle de mélange permet également une certaine flexibilité lors de l'exploration de l'espace des paramètres, en donnant la possibilité de favoriser ou d'exclure facilement une zone prédéfinie. Les résultats d'estimation obtenus sont satisfaisants pour les exemples synthétiques et corrects pour les cas expérimentaux, mais l'exécution de la chaîne de calcul a clairement mis en évidence la forte dépendance de la durée de calcul avec le nombre d'appels au modèle de dispersion. \\

Dans le chapitre 4, nous proposons une solution à ce problème, en substituant au modèle direct une version rétrograde pour simuler la dispersion lors du calcul des matrices source-récepteur. Cette modification permet de soustraire au processus d'estimation toute la charge de calcul due au modèle de dispersion, en pré-calculant sur un maillage couvrant tout le domaine les valeurs requises pour la création des matrices source-récepteur avant de lancer l'algorithme AMIS. Cela est rendu possible grâce au principe de dualité direct-rétrograde, qui permet lors de l'estimation de la source, de remplacer les concentrations calculées pour chaque particule par des rétro-concentrations préalablement disponibles sur le domaine maillé. Les calculs de rétro-propagation sont également utilisés pour une initialisation améliorée de la loi de proposition, afin d'orienter l'AMIS vers une zone d'intérêt à explorer dès la première itération. Ces améliorations ont permis d'obtenir de bons résultats, à la fois dans un milieu rural et dans une configuration urbaine plus complexe, avec l'utilisation d'un modèle de dispersion lagrangien rétrograde intégré au système Parallel Micro-SWIFT-SPRAY (PMSS). Les différentes expériences menées sur les cas-tests ont notamment illustré l'influence de la disposition du réseau de capteurs, ainsi que l'impact des valeurs prises par les variances a priori sur le débit d'émission et d'observation. Il faut toutefois noter que ces valeurs nécessitent d'être arbitrairement choisies avant de lancer l'algorithme d'estimation et il n'existe pas, dans l'état actuel des choses, de formule générique permettant de leur affecter des valeurs par défaut. Afin de pallier à cette limitation, une amélioration à envisager serait d'intégrer directement ces paramètres à l'ensemble des valeurs à reconstruire par l'algorithme d'estimation.

\section{Perspectives}

En plus des résultats précédemment mentionnés, la synthèse des travaux exposés dans ce manuscrit permet de définir un certain nombre de pistes intéressantes à explorer.\\

Une suite logique du chapitre 4 serait l'application de la méthodologie AMIS à des cas expérimentaux en présence d'obstacles. Pour cela, il existe plusieurs jeux de données de référence, dont la campagne Mock Urban Setting Test (MUST) \cite{Yee2004}. Celle-ci, menée en 2003, a consisté à mesurer, avec des appareils de haute précision, les variables météorologiques et les concentrations lors d'un rejet de gaz traceur dans un environnement où des obstacles (sous la forme de conteneurs) sont disposés de façon régulière. L'utilisation des méthodes d'estimations développées dans ce manuscrit permettrait ainsi de comparer les performances de l'AMIS à celles d'une approche de type MCMC, grâce à l'exploitation des résultats proposés par \cite{Keats2007} sur le même cas d'application. A partir de là, on peut également envisager des tests dans le cas expérimental urbain, avec par exemple l'utilisation d'une campagne de mesures telle que Joint Urban 2003 \cite{Allwine2004}, où le rejet de gaz traceur a été effectué dans le centre de la ville d'Oklahoma City (Etats-Unis).\\

Un autre aspect mentionné dans ce manuscrit est celui du dimensionnement d'un réseau de capteurs. Celui-ci peut être associé à la démarche d'estimation du terme source en utilisant cette dernière comme critère d'optimalité pour l'établissement d'un ensemble de points de mesure. Cela pourrait alors constituer un prolongement intéressant des travaux de cette thèse, en analysant de façon plus approfondie l'influence de la densité des capteurs ainsi que de leur résolution temporelle sur la qualité de l'estimation obtenue. Les résultats obtenus permettraient alors: 
\begin{itemize}
	\item de concevoir entièrement un réseau approprié à une situation donnée: par exemple en définissant les zones à instrumenter dans un complexe industriel afin de détecter et d'estimer le plus rapidement possible les paramètres d'une éventuelle fuites, ou en situation opérationnelle, de placer des capteurs mobiles sur un domaine affecté par un rejet;
	\item sur un réseau existant, d'améliorer le positionnement des capteurs, voire d'en rajouter dans des zones stratégiques où une meilleure instrumentation permettrait une identification plus efficace d'un rejet potentiel.
\end{itemize}

Les méthodes de dimensionnement du réseau de mesures sont déjà exploitées dans le cadre de l'assimilation de données (voir notamment \cite{Abida2009}) et constituent ainsi un volet d'étude complémentaire à l'estimation du terme source.\\

Enfin, le dernier volet prospectif concerne le passage de l'algorithme sur un mode séquentiel. En effet, dans la pratique, il est possible que les informations issues des capteurs ne soient pas immédiatement disponibles, mais deviennent progressivement accessibles au cours du temps. Dans l'état actuel des choses, l'ajout d'éléments supplémentaires au vecteur d'observation nécessite de relancer l'algorithme AMIS depuis le début, ce dernier ne pouvant traiter les observations que de façon "statique". Un moyen de mieux tenir compte d'un caractère "dynamique" de l'acquisition des mesures consisterait à substituer l'AMIS par un nouvel algorithme basé sur le modèle du \textit{SMC sampler} introduit dans \cite{Delmoral2006}. En suivant cette méthodologie, il devient possible d'évaluer séquentiellement une suite de distributions de probabilités, et d'ajuster les éléments de cette suite grâce à l'utilisation d'un noyau de transition rétrograde lorsque le vecteur d'observation est mis à jour. Concrètement, cela permet de réutiliser l'information obtenue par les échantillons qui ont été tirés avant le changement du vecteur d'observation, au lieu de devoir reprendre le calcul depuis la première itération en ne tenant plus compte des résultats précédemment obtenus. L'apport de cette approche est présenté dans \cite{Nguyen2016}:les avantages qui en découlent constituent ainsi une perspective intéressante pour l'application à l'estimation du terme source.


%Une autre perspective serait d'aborder le problème d'inférence bayésienne sous une forme séquentielle. C'est notamment le cas lorsque la totalité des mesures issues des capteurs n'est pas immédiatement disponible, mais plutôt acquise au fil du temps: une approche séquentielle permettrait de lancer la recherche des paramètres de la source sur les observations existantes et d'intégrer progressivement les nouvelles observations. Une telle démarche permettrait ainsi de raffiner l'estimation à chaque nouvel ajout d'information provenant des capteurs. Cette méthodologie peut également être exploitée lorsqu'on dispose d'un vecteur d'observations de dimension très grande: en le traitant de façon séquentielle lors du processus d'estimation, on se ramène à un problème de moindre dimension, ce qui f
