\afterpage{\blankpage}

	\mainmatter
\chapter{Introduction}

\section{Pourquoi chercher à reconstruire des sources de pollution?}
	La menace de rejets de substances \textit{Nucléaires}, \textit{Radiologiques}, \textit{{Biologiques}} ou \textit{Chimiques} (NRBC) dans l'atmosphère suscite un fort intérêt, du fait des enjeux humains et environnementaux qu'ils affectent. De tels incidents peuvent être d'origine accidentelle, causés par des rejets ayant eu lieu dans des sites industriels stockant ou exploitant des matières dangereuses. Plusieurs événements historiques témoignent de l'impact de tels accidents :\\
	\begin{itemize}
		\item Seveso (Italie) en 1976: un rejet accidentel de dioxine provenant d'une usine chimique engendre un nuage toxique contaminant une surface de près de 2.8 km$^2$, touchant plus de 400 personnes victimes de lésions cutanées, et nécessitant l'abattage de près de 80 000 bêtes contaminées dans les domaines agricoles atteints \cite{Seveso1976}. 
		
		\item Bhopal (Inde) en 1984: une explosion dans une usine de pesticides entraîne un important rejet de substances chimiques toxiques (isocyanathe de méthyle, cyanure hydrogéné) touchant directement la population vivant aux alentours. Le bilan {à} long terme est d'au moins 16 000 morts et d'environ 500 000 intoxiqués \cite{Bhopal1984}.
		
		\item Tchernobyl (Ukraine) en 1986: suite à des erreurs humaines lors d'opérations sur un réacteur de la centrale nucléaire locale, le c\oe{}ur est entré en fusion: il s'en est suivi une explosion et la libération d'importantes quantités d'éléments radioactifs dans l'atmosphère. Le nuage formé par ces polluants s'est répandu à l'échelle continentale sur une grande partie de l'Europe \cite{Repussard2006}.
		
		\item Algésiras (Espagne) en 1998: une usine espagnole incinère accidentellement une source radioactive dans ses hauts-fourneaux. Ce n'est que près de trois semaines plus tard que l'origine de la fuite est établie. Sur la base des mesures effectuées et de la reconstitution des courants atmosphériques, la quantité totale de césium 137 libérée a été évaluée à 1850 GBq \cite{Estevan2003}.
		
		\item Fukushima (Japon) en 2011: suite au déclenchement d'un séisme de magnitude 9.0 au large des côtes japonaises, les dégâts causés par le tsunami induit ont entraîné la fusion d'au moins deux réacteurs de la centrale de Fukushima, causant ainsi d'importants rejets radioactifs sur le territoire japonais, et plus largement, sur une large portion de l'océan Pacifique \cite{IRSN2012}.
		
		\item Igualada (Espagne) en 2015: une citerne explose en effectuant une livraison dans une usine chimique, causant la propagation d'un nuage orange d'acide nitrique. L'incident a entraîné des mesures de confinement des populations dans le voisinage immédiat de l'usine.
		\item Los Angeles (Etats-Unis) en 2015: une fuite localisée dans un puits de stockage de gaz de ville entraîne d'importants rejets de méthane dans l'atmosphère. L'incident a été déclaré le 23 octobre 2015, et annoncé être "sous contrôle" le 11 février 2016 par l'entreprise gestionnaire du puits. Durant la période de rejet, entre 30 et 50 tonnes par heure de méthane furent rejetés dans l'air. Le méthane étant un gaz à effet de serre, les conséquences environnementales à moyen et long terme sont déjà considérées comme graves.\\
	\end{itemize}
	
	Les incidents NRBC peuvent aussi être issus d'actes malveillants relevant du terrorisme. Ce fut le cas à Tokyo (Japon) en 1995, où des membres d'une secte ont percé des poches contenant du gaz sarin (un puissant neurotoxique) dans des rames de métro. Le bilan final {fut}  de 12 morts et plus de 5500 blessés. Plus récemment, le risque d'attentats NRBC a également été mis en évidence  en France suite aux attentats de l'année 2015, et amplifié en raison du contexte géopolitique actuel.\\
	
	Dans tous les cas, il est vital de disposer de techniques rapides pour détecter et évaluer le risque, afin d'assurer au mieux la sécurité des personnes et de coordonner les manoeuvres des équipes de premier secours. De telles techniques reposent sur des méthodes de modélisation des phénomènes physiques régissant l'atmosphère, ainsi que sur un système d'instrumentation permettant, entre autres, de caractériser et quantifier la présence de substances toxiques dans l'air.\\
	
	Cependant, pour que ces outils de modélisation puissent fonctionner, il est indispensable de disposer d'un certain nombre de paramètres, dont les caractéristiques de la source à l'origine du rejet. C'est ce point particulier que nous illustrerons dans la suite de ce chapitre, après un bref exposé introductif sur la physique de la dispersion atmosphérique.\\
	
	\clearpage

	\section{Les principes de la dispersion atmosphérique}
	
	Nous décrivons ici les grandes lignes des règles physiques qui régissent la propagation d'un nuage de polluant dans l'atmosphère. Pour un exposé plus exhaustif, le lecteur peut se référer à \cite{Sportisse2008}.
	
	\subsection{L'équation d'advection-diffusion}
	
	Une fois qu'un polluant est émis dans l'atmosphère, son comportement est régi par plusieurs processus distincts:\\
	\begin{enumerate}
		\item le \textbf{transport}, ou \textbf{advection} qui se fait sous l'influence des circulations d'air dans l'atmosphère,
		\item la \textbf{diffusion}, résultant de la nature turbulente des écoulements dans la partie basse de l'atmosphère (couche limite),
		\item les processus de \textbf{pertes par dépôt sec ou humide}, diminuant la quantité de polluant transportée,
		\item les éventuelles \textbf{transformations physico-chimiques} pouvant altérer l'état du polluant lors de son séjour dans l'atmosphère: la filiation radioactive (s'il s'agit d'un radionucléide), ou les diverses réactions chimiques pouvant avoir lieu avec les autres composants de l'air.\\
	\end{enumerate}
	
	Nous supposerons en première approximation que les processus (3) et (4) énumérés précédemment ne sont pas pris en compte. Si on considère le transport d'un polluant unique dont la concentration peut être décrite au point $\bm{x} = (x,y,z) \in \mathbb{R}^3$ et à l'instant $t$ par une fonction $C(\bm{x},t)$. On peut alors écrire la loi de conservation de la masse pour $C$ sous la forme suivante:
	
	\begin{equation}
	\label{eqn_conservation_masse}
		\dfrac{\partial C}{\partial t} + \nabla \cdot \vec{J}(\bm{x},t) = \VecSigma
	\end{equation}
	où $\VecSigma$ est le \textit{terme source}, $\vec{J}(\bm{x},t)$ représente le flux de masse du polluant, et $\nabla$ désigne l'opérateur gradient. $\vec{J}$ est une fonction vectorielle qui regroupe la somme des phénomènes distincts d'advection et de diffusion: 
	\begin{equation}
	\label{eqn_somme_flux}
	\vec{J} = \vec{J}_A + \vec{J}_D
	\end{equation}
	Le terme $\vec{J}_D$ est associé au phénomène de diffusion. Celui-ci est généralement considéré comme suivant la première loi de Fick, stipulant que le flux de diffusion $\vec{J}_D$ est proportionnel au gradient de concentration : 
	
	\begin{equation}
	\label{eqn_fick_diffusion}
	\vec{J}_D = - \bm{K}\nabla C
	\end{equation}
	où $\bm{K}$ est une matrice contenant les coefficients de diffusion moléculaire, qui dépendent de l'espèce du polluant. Le terme $\vec{J}_A$ est associé au phénomène d'advection, et traduit une dépendance linéaire de la concentration par rapport au champ de vent $\bm{\vec{u}}$:
	
	\begin{equation}
	\label{eqn_flux_advection}
	\vec{J}_A = C\bm{\vec{u}}
	\end{equation}
	La combinaison des équations \eqref{eqn_somme_flux}, \eqref{eqn_fick_diffusion} et \eqref{eqn_flux_advection} mène ainsi à la formulation de l'\textit{équation d'advection-diffusion}, formulée sur le modèle de \cite{Stockie2011}:\\
	
	\begin{equation}
		\label{eqn_advection_diffusion}
		\dfrac{\partial C}{\partial t} + \nabla \cdot(C\bm{\vec{u}}) = \nabla \cdot (\bm{K}\nabla C) + \VecSigma
	\end{equation}
	Cette équation constitue la base de départ pour les différents modèles numériques cherchant à simuler les phénomènes de dispersion atmosphérique.\\
	
	\subsection{Les différents modèles de dispersion}
	
	Suivant les situations, la taille du domaine sur lequel les calculs sont effectués peut grandement différer. On distingue habituellement : \\
	\begin{itemize}
		\item l'\textit{échelle locale}, aussi appelée \textit{micro-échelle} (jusqu'à 1 km): à ce niveau, des phénomènes spécifiques tels que la présence de bâtiments doivent être pris en compte dans le calcul des champs de vent. C'est à cette échelle qu'il convient de se placer pour traiter les études d'impact en milieu urbain (par exemple à l'échelle d'un quartier), ou sur un site industriel. 
		\item la \textit{méso-échelle}, ou \textit{échelle régionale} (jusqu'à 1000 km): elle est utilisée pour travailler sur des phénomènes plus larges, par exemple pour étudier l'impact de la pollution à l'ozone ou aux particules fines sur le territoire d'un pays tel que la France. 
		\item l'\textit{échelle planétaire}, ou \textit{échelle synoptique} ($>$ 1000 km): on s'intéresse ici à l'impact d'événements de très grande ampleur, tels que les {accidents} de Tchernobyl ou Fukushima. \\
		
	\end{itemize}
	
	On peut également noter la corrélation entre les échelles spatiales et temporelles dans le cadre accidentel: au niveau local, on s'intéresse typiquement à des phénomènes n'excédant pas quelques heures, alors qu'à l'échelle du globe on se place sur un intervalle de plusieurs semaines, voire plusieurs mois.\\ 
	
	Pour couvrir efficacement ces différentes tailles de zones, plusieurs types de modèles de dispersion existent. Ceux-ci se divisent en quatre catégories principales:\\
	
	\begin{itemize} 
		
	\item \textbf{Les modèles gaussiens}:
	Sous certaines hypothèses simplificatrices, l'équation \eqref{eqn_advection_diffusion} peut donner des solutions analytiques pour déterminer la concentration de polluant au sein de \textit{panaches} ou de \textit{bouffées} selon la configuration choisie. \cite{Stockie2011} résume les principes régissant les modèles gaussiens, et une présentation plus détaillée de leur fonctionnement sera introduite au Chapitre 3 du présent manuscrit.\\
	
	\item \textbf{Les modèles eulériens}:
	Le domaine de simulation sur lequel est résolue l'équation \eqref{eqn_advection_diffusion} est discrétisé en un maillage de calcul  par des méthodes numériques. On retrouve ce type d'approche dans des études de cas à l'échelle continentale \cite{Saunier2013} ou globale. \\
	
	\item \textbf{Les modèles lagrangiens particulaires}:
	Dans le cadre lagrangien, le rejet est modélisé comme un ensemble de particules numériques porteuses d'une masse élémentaire. Le modèle suit la trajectoire de chaque particule, dont le mouvement moyen est constitué d'une composante régie par le champ de vent, et d'une composante stochastique traduisant la variabilité causée par la turbulence. La concentration mesurée sur un volume élémentaire du domaine à un temps donné est ainsi égale à la somme des masses élémentaires portées par chaque particule contenue dans ce volume à cet instant. Ce type de modèle est présenté plus en détails au Chapitre 4.\\
	
	\item \textbf{Les modèles de mécanique des fluides}:
	Aussi appelés \textit{Computational Fluid Dynamics} (CFD), ces modèles sont utilisés à petite échelle, et impliquent une résolution des équations de Navier-Stokes sur un maillage relativement fin. Il en résulte une {très} bonne précision des calculs, en particulier en cas de présence d'obstacles menant à des écoulements complexes.\\
	
	\end{itemize}
	
	
	\section{Les caractéristiques du terme source}
	
	Les modèles de dispersion présentés dans le paragraphe précédent nécessitent plusieurs types de données d'entrée: 
	\begin{itemize}
		\item un certain nombre de \textbf{paramètres météorologiques}: dans le cas le plus simple, on considère la direction et la vitesse du vent mesurés par une station au sol dans le domaine d'étude ou à sa proximité, ainsi que la stratification atmosphérique. Ces données sont alors supposées homogènes. Dans des cas plus complexes, plusieurs instruments météorologiques peuvent être pris en compte, au sol comme en altitude. Enfin, il est également possible d'exploiter les résultats d'un système de prévision météorologique: les grandeurs considérées sont alors des champs de vitesse et direction de vent, température, humidité, nébulosité et flux de rayonnement.
		\item les \textbf{paramètres de la source} à l'origine des émissions à modéliser{;}
	\end{itemize}
	
	C'est sur ce second point que nous allons plus particulièrement nous pencher dans la suite de ce paragraphe. En effet, la terminologie employée pour décrire la source d'un rejet polluant peut parfois prêter à confusion, car il existe plusieurs façons d'en décrire les caractéristiques. Nous fixons ici les termes utilisés pour distinguer les différents types de source rencontrés.\\
	
	Une source est dite \textit{localisée} si sa position peut être réduite à un point de l'espace, on peut également parler de \textit{source ponctuelle}. Par opposition, une \textit{source étendue} est caractérisée par une surface d'émission, il peut, par exemple, s'agir d'un bac de décantation dans une usine de retraitement des déchets, si on se place à une échelle suffisamment petite.\\
	
	Une source est également définie par le profil temporel de son rejet, autrement dit la variation de la quantité de polluant rejetée par unité de temps. Comme nous travaillons dans un cadre de modélisation numérique d'un phénomène physique, il est nécessaire de définir une discrétisation temporelle sur $T_s$ instants d'émission, sous la forme d'un vecteur $\bm{t'}_s = (t'_1, \dots, t'_{T_s})$. On peut alors construire le vecteur du profil d'émission comme étant la liste des quantités rejetées par pas de temps d'émission (figure \ref{fig_courbe_profil_source}) : 
	
	$$ \VecQSource = \left(q(t'_1), q(t'_2), \dots, q(t'_{T_s})\right)$$
	
	Si le rejet est suffisamment bref pour n'être effectif que sur un unique pas de temps, alors $\VecTSource$ est réduit à un scalaire $t'_s$, et $\VecQSource$ à une masse totale émise $q_s$ : dans ce cas, on parle d'une source dite \textit{instantanée}. Dans le cas inverse, et lorsque le débit n'est pas constant, on parle d'une source \textit{non-instantanée}. Si $ \forall i \in \{1, \dots, T_s\}, ~ q(t'_i)$ est constant, la source est dite \textit{continue}. \\
	
	
	\begin{figure}[hb]
		\centering
		\includegraphics[scale=0.6]{courbe_profil_source.png}
		\caption{Exemple de discrétisation d'une source non-instantanée.}
		\label{fig_courbe_profil_source}
	\end{figure}
	
	Enfin, un cas d'étude peut comporter une \textit{source unique}, ou bien avoir une configuration \textit{multi-sources}.\\
	

	\section{Estimation du terme source: un état de l'art}
	\label{section_etat_art_STE}
	
	\subsection{Une brève introduction aux problèmes inverses}
	Le domaine des problèmes inverses constitue un vaste champ de la littérature scientifique, et de nombreux travaux y ont été menés. En pratique, résoudre un problème inverse revient à reconstituer un signal, une image, ou plus généralement une donnée non-observable à partir de mesures existantes, appelées observations.
	Cette approche est duale à celle du problème direct où, à partir d'un signal ou d'un ensemble de paramètres initiaux, on cherche à en calculer les effets après une transformation donnée (figure \ref{fig_diagramme_direct_inverse}).
	De nombreux domaines théoriques et pratiques font appel aux méthodes inverses. Le lecteur pourra trouver une revue complète de ces applications dans \cite{Tarantola2004}, on peut en citer quelques exemples tels que la géophysique \cite{Backus1967}, l'acoustique \cite{Kirsch1988}, l'imagerie satellite \cite{Park2003} et médicale \cite{Arridge1999}, les transferts thermiques \cite{McCormik1992} ou encore la finance quantitative \cite{Dembo1999}.\\
	
	\begin{figure}
		\centering
		\includegraphics[scale=0.5]{diagramme_direct_inverse.png}
		\caption{Schéma de principe illustrant la dualité entre problèmes direct et inverse.}
		\label{fig_diagramme_direct_inverse}
	\end{figure}
	
	
	En dispersion atmosphérique, le problème direct peut ainsi se traduire par la donnée des paramètres de la source au modèle de dispersion, qui va produire le champ de concentration résultant. Cela revient bien à définir le problème inverse comme étant la reconstruction des paramètres de la source à partir des mesures de concentrations aux capteurs. Nous décrivons dans les paragraphes suivants les différentes approches existant dans la littérature qui permettent de résoudre ce problème. Des éléments complémentaires sur la question sont également disponibles dans \cite{Rao2007}. Dans un souci de brièveté, par la suite nous qualifierons de "problèmes STE" (\textit{source term estimation}) toutes les études de cas visant à estimer les paramètres d'une ou de plusieurs sources inconnues.\\
	
	
 \subsection{{Rétro-transport et modèles rétrogrades}}
 
 Une première possibilité consiste à étudier le problème dual associé à l'équation d'advection-diffusion  \eqref{eqn_advection_diffusion}, en inversant la flèche du temps, et par conséquent la direction du vent.  Par le principe de symétrie présenté dans \cite{Hourdin2006a}, on peut alors introduire la notion de \textit{rétro-transport} ou \textit{backtracking} en  réécrivant l'équation d'advection-diffusion sous la forme : 
 
 \begin{equation}
 \label{eqn_advection_diffusion_backward}
 \dfrac{-\partial C^*}{\partial t} - \nabla \cdot(C^*\bm{\vec{u}}) = \nabla \cdot (\bm{K}\nabla C^*) + \VecObs
 \end{equation}
 
 où $C^*$ est un champ de concentrations conjuguées, ou \textit{rétro-concentrations}, dont les valeurs sont obtenues par des rétro-rejets virtuels depuis les capteurs vers la source. Le modèle de dispersion associé à cette démarche duale est alors appelé {\textit{modèle de rétro-dispersion}, ou \textit{modèle rétrograde}}. Cette démarche revient ainsi à transformer le champ d'émission {obtenu par l'équation \eqref{eqn_advection_diffusion}} en un champ de rétro-émissions issues des capteurs, comme illustré sur les figures \ref{schema_probleme_direct} et \ref{schema_probleme_inverse}, et ainsi fournir une estimation du terme source. Dans \cite{Flesch1995}, un modèle {rétrograde} lagrangien est construit pour estimer le profil de rejet d'une source surfacique.  Cette méthode est également appliquée dans \cite{Pudykiewicz1998}, où la position et l'intensité du terme source de Tchernobyl sont reconstruits. 
 
 \newpage De même, dans \cite{Hourdin2006b} le principe du \textit{backtracking} est appliqué à un modèle eulérien pour retrouver la source de la campagne ETEX\footnote{L’expérience \textit{European Tracer EXperiment 1} (ETEX-1), menée en 1994,  a consisté à mesurer et étudier l'impact à l'échelle européenne de rejets de gaz traceurs passifs émis depuis une source située dans la ville de Monterfil, en Bretagne.}.\\
 
 \begin{figure}[h!]
 	\begin{subfigure}{0.5\textwidth}
 		\includegraphics[scale=0.6]{schema_probleme_direct.png}
 		\caption{Approche orientée source}
 		\label{schema_probleme_direct}
 	\end{subfigure}
 	\begin{subfigure}{0.5\textwidth}
 		\includegraphics[scale=0.2]{schema_probleme_inverse.png}
 		\caption{Approche orientée récepteur}
 		\label{schema_probleme_inverse}
 	\end{subfigure}
 	\caption{Exemple illustrant la relation entre une source unique $S$ et trois capteurs $R_1,R_2,R_3$ sur un problème en deux dimensions pour les approches en "direct" (\ref{schema_probleme_direct}) et en "adjoint" (\ref{schema_probleme_inverse}). }
 \end{figure}

 
 \subsection{Formulation linéaire du problème et optimisation}
 \label{subsection_MCO}
 
Le modèle direct de simulation des concentrations résultant d'un rejet de polluant peut être décrit de façon linéaire. Pour cela, on définit de façon arbitraire un maillage dans le temps et l'espace qui couvre respectivement la période d'observation et le domaine spatial considérés. En pratique, cela revient à discrétiser l'espace en $N_xN_yN_z$ points, chaque point étant associé à un profil d'émission de longueur $T_s$. Dans un tel contexte, le terme source est défini comme un vecteur $\VecSigma \in \mathbb{R}^{N_\VecSigma}$ (où $N_\VecSigma = N_xN_yN_zT_s$). Il est ainsi possible de lier les observations générées par le modèle et le vecteur source $\VecSigma$ par la formule suivante, appelée \textit{relation source-récepteur}:

\begin{equation}
	\label{eq_relation_SR_non_parametrique}
	\VecObs = \bm{H}\VecSigma + \bm{\varepsilon}
\end{equation}
où $\VecObs \in \mathbb{R}^m$ représente le vecteur des observations, $\VecSigma \in \mathbb{R}^{N_\VecSigma}$ la source discrétisée et $\bm{H} \in \mathbb{R}^{m \times N_\VecSigma}$  la \textit{matrice de transfert}. Le rôle de $\bm{H}$ est d'assurer la projection depuis l'espace $\mathbb{R}^{N_\VecSigma}$  de la source vers l'espace $\mathbb{R}^m$ des observations grâce à l'utilisation d'un modèle de dispersion atmosphérique. $\bm{\varepsilon} \in \mathbb{R}^m$ est le vecteur des erreurs d'observation, et prend en compte les erreurs de mesure, de représentativité et de modèle. \\

Retrouver le terme $\VecSigma$ dans l'équation \eqref{eq_relation_SR_non_parametrique} revient alors à résoudre un problème inverse linéaire: il existe pour cela différentes méthodes que nous détaillons ci-après.

\subsubsection{Définition de la fonction-coût à optimiser}

L'équation \eqref{eq_relation_SR_non_parametrique} peut être réécrite sous la forme : 
 
 \begin{equation}
 \VecObs = \widehat{\VecObs} + \VecErreur
 \label{eq_relation_SR_nonparam_simple}
 \end{equation}

où $\widehat{\VecObs} = \MatH \VecSigma$ est l'approximation des observations $\VecObs$ par le processus de modélisation. On voit bien que dans le cas parfait où ce dernier reproduit exactement les observations attendues, on a $\VecErreur = 0$ et par suite $\widehat{\VecObs} = \VecObs$. Le fait de chercher la valeur $\VecSigma$ pour laquelle $\widehat{\VecObs}$ se rapproche le plus de $\VecObs$ permet ainsi de définir le problème STE sous la forme d'une minimisation de l'erreur $\VecErreur$: on définit pour cela une fonction-coût qui s'écrit : 

\begin{equation}
\CostF(\VecSigma) = ||\VecObs - \MatH \VecSigma||_p = ||\VecObs- \widehat{\VecObs}||_p
\label{eq_cost_function_definition}
\end{equation}
où $||\cdot||_p$ désigne une norme $L_p$ arbitrairement choisie. La résolution du problème STE se traduit alors par la minimisation de cette fonction-coût:

\begin{equation}
\widehat{\VecSigma} = \argmin_\VecSigma \mathcal{J}(\VecSigma)
\label{eq_argmin_cost_function}
\end{equation}
où $\widehat{\VecSigma}$ est l'estimation du terme source recherchée.

\subsubsection{Minimisation d'une fonction-coût quadratique}

Une des approches les plus courantes pour résoudre l'équation \eqref{eq_argmin_cost_function} consiste à choisir $p=2$, autrement dit à minimiser l'écart quadratique entre les observations $\VecObs$ et les données générées par le modèle direct. Concrètement, cela revient à optimiser la fonction-coût suivante:

\begin{equation}
\CostF_{2}(\VecSigma) = ||\VecObs - \MatH \VecSigma ||_2^2
\label{eq_cost_MCO}
\end{equation}

Le fait de minimiser $\CostF_2$ revient à appliquer la méthode des \textit{moindres carrés} pour résoudre le problème inverse. 

Le terme d'erreur que nous cherchons à minimiser est une combinaison des différentes sources d'incertitudes existantes (mesure, représentativité, modèle). Comme il est difficile de quantifier les contributions respectives de ces dernières, on peut invoquer le principe du maximum d'entropie pour définir la représentation statistique de l'ensemble des incertitudes comme étant un bruit gaussien centré et de matrice de covariance $\MatR$ : 
\begin{equation}
\VecErreur \sim \mathcal{N}(0,\MatR)
\label{eq_bruit_obs}
\end{equation}

Dans le cadre de statistiques gaussiennes, la fonction-coût peut alors s'écrire sous la forme suivante \cite{Winiarek2011}:

\begin{equation}
\mathcal{J}_2(\VecSigma) = \dfrac{1}{2}(\VecObs - \MatH \VecSigma)^T \MatR^{-1}(\VecObs - \MatH\VecSigma)
\label{eq_cost_forme2}
\end{equation}

{Dans l'hypothèse où les éléments du vecteur de bruit sont indépendants et identiquement distribués, alors la matrice $\MatB$ est diagonale:}
	
	\begin{equation}
	{
		\MatB = \sigma_R^2 \MatId
	}
	\end{equation}
{où $\sigma_R^2$ est la variance d'erreur.} Plusieurs travaux dans la littérature ont recours a cette formulation: \\
\begin{itemize}
	\item Dans \cite{Kathirgamanathan2002}, un point source instantané est estimé à l'aide d'une méthode des moindres carrés, cette source étant caractérisée par sa masse totale émise, sa position et son instant d'émission. L'étude qui y est menée met notamment l'accent sur l'influence du dimensionnement du réseau de capteurs sur la performance de la reconstruction : au moins trois points de mesure sont nécessaires, et la distance entre les différents capteurs influe sur la qualité de l'estimation.\\
	
	\item Dans \cite{Ryall2001}, ce sont des zones d'émission plus grandes qui sont estimées pour des rejets de gaz à effet de serre sur plusieurs années. \\
	
	\item Dans \cite{Matthes2005}, une approche en deux temps est formulée afin de résoudre un problème de complexité proportionnellement croissante au nombre de capteurs. Dans une première étape, les mesures individuelles de chaque capteur produisent des ensembles de positions probables pour la source. Dans un deuxième temps, ces ensembles sont comparés pour estimer la meilleure position en faisant varier l'intensité de la source à retrouver. \\
	
	\item  Dans \cite{Robertson1998}, l'inversion est faite dans le cadre de l'assimilation de données: cette méthodologie permet de corriger de façon variationnelle les paramètres du modèle de dispersion selon une boucle de rétro-action basée sur des observations. Très utilisée en météorologie, l'assimilation de données agit en deux temps selon un principe de "prévision-correction", et itère sur le cycle suivant:
	\begin{enumerate}
		\item  on calcule d'abord une ébauche de l'état à l'instant présent $t$, en appliquant un modèle de prévision en $t-1$,
		\item  on exploite ensuite les observations disponibles à l'instant $t$ pour corriger les paramètres du modèle en les comparant avec l'ébauche. \\
	\end{enumerate}
	
	 \item \cite{Issartel2005} mentionne le principe d'\textit{illumination}, qui est une mesure quantitative de la représentativité des mesures dans le temps et l'espace. Dans le cadre d'un modèle adjoint, l'illumination permet ainsi de caractériser des zones qui ne sont pas forcément couvertes par les rétro-émissions issues des capteurs, par exemple si la zone considérée est relativement éloignée {de tout point de mesure}. Cependant, si l'information d'illumination est utilisée pour inverser un terme source, alors elle aura tendance à fortement favoriser les solutions proches des capteurs. Pour compenser ce problème, une phase de \textit{renormalisation} est introduite (\cite{Issartel2007} et \cite{Sharan2009}) pour équilibrer l'information apportée par chacun des capteurs. D'abord utilisée à grande échelle, cette méthodologie a récemment été validée à l'échelle locale dans \cite{Singh2014}, et dans un contexte urbain par \cite{Kumar2015}. \\

\end{itemize}

\subsubsection{Régularisations}

Bien souvent, le nombre d'observations à notre disposition est limité, à cause par exemple d'un faible nombre de capteurs ou d'une mauvaise représentativité temporelle dans les mesures fournies par ces derniers. Dans de telles situations, le problème inverse peut admettre une infinité de solutions, et ainsi devenir \textit{mal-posé}. Rappelons qu'un problème \textit{bien posé} au sens de Hadamard \cite{Hadamard1902} désigne un modèle mathématique dont la solution existe, est unique, et dépend de façon continue des données. \\

A l'inverse, un problème mal-posé déroge à au moins une des règles précédemment citées. \\

De nombreux problèmes physiques sont intrinsèquement mal-posés, et des solutions existent pour les \textit{régulariser}, c'est-à-dire leur appliquer une transformation qui permet de revenir à un problème bien posé. Parmi ces méthodes, la \textit{régularisation de Tikhonov} \cite{Tikhonov1963} est la plus fréquemment employée: elle consiste à ajouter un terme $\lambda_B ||\VecSigmaB||_2$ dit \textit{régularisant} à l'expression de la fonction-coût $\CostF_2$, qui devient alors:

 \begin{equation}
 {
 \mathcal{J}_{2_B}(\VecSigma) = ||\VecObs - \MatH \VecSigma||_2^2 + \lambda_B ||\VecSigma||_2
 \label{eq_cost_MCO_reg}
}
 \end{equation}
 
 Le rôle du terme régularisant est de pénaliser les solutions indésirables, afin de garantir le critère d'unicité de la solution du problème inverse.  Dans le cas des statistiques gaussiennes, l'erreur  $\VecSigma - \VecSigmaB$ suit également une loi normale centrée, de matrice de covariance $\MatB$:
  
  \begin{equation}
  (\VecSigma - \VecSigmaB) \sim \mathcal{N}(0,\MatB)
  \label{eq_erreur_ebauche}
  \end{equation}
  
  L'équation \eqref{eq_cost_forme2} devient alors \cite{Winiarek2011}: 
 
\begin{equation}
\mathcal{J}_{2_B}(\VecSigma) = \dfrac{1}{2}(\VecObs - \MatH \VecSigma)^T \MatR^{-1}(\VecObs - \MatH\VecSigma) + \dfrac{1}{2}(\VecSigma - \VecSigmaB)^T\MatB^{-1}(\VecSigma - {\VecSigma_b})
\label{eq_cost_reg_forme2}
\end{equation}

Dans la littérature des méthodes d'estimation du terme source, le terme régularisant est qualifié d'\textit{ébauche}, de \textit{first-guess}, ou encore de \textit{background}. Les travaux présentés dans \cite{Winiarek2012} introduisent une approche basée sur la résolution de l'équation \eqref{eq_cost_reg_forme2} via la méthode du \textit{Best Linear Unbiased Estimator} (BLUE), qui vise à annuler le gradient de la fonction-coût cible. Une approche similaire est proposée par \cite{Saunier2013}, élaborant une reconstruction du terme source de l'accident de Fukushima en deux temps. Tout d'abord, une première fonction-coût est minimisée pour estimer la période du rejet, puis tenant compte de cette contrainte, une seconde fonction-coût est optimisée pour retrouver la localisation de la source. 

Il est intéressant de remarquer que l'équation \eqref{eq_cost_reg_forme2} correspond à celle de la méthode d'assimilation variationnelle de données 3D-var \cite{Courtier1998}. La vision de ce type d'estimation correspond  ainsi à celle de l'assimilation des concentrations mesurées par les capteurs dans le but de reconstruire un état inconnu, qui est ici le champ d'émission de la source recherchée. Notons également que des études ont été menées dans le cadre de statistiques non-gaussiennes, en particulier dans les cas où on cherche à caractériser l'ébauche suivant certaines hypothèses spécifiques \cite{Bocquet2005a}. Ainsi, les auteurs de \cite{Krysta2007} font l'hypothèse d'une source ponctuelle et instantanée pour définir une forme particulière d'ébauche, et utilisent un principe de maximum d'entropie sur la moyenne \cite{Jaynes1957} pour réécrire la fonction-coût et en dériver un estimateur approprié pour la source qu'ils recherchent.\\

Il peut exister des situations où les observations prennent l'allure de \textit{signaux parcimonieux} (ou \textit{sparse signals}), autrement dit elles sont décrites par un très faible nombre de valeurs non-nulles sur l'intervalle temporel de mesure. Pour tenir compte de cela, l'équation \eqref{eq_cost_MCO} est modifiée, et c'est cette fois un terme régularisant sous une norme $L_1$ qui est utilisé: 

\begin{equation}
\mathcal{J}_{2_L} = ||\VecObs - \MatH\VecSigma||_2^2 + \lambda_L ||\VecSigma||_1
\label{eq_MCO_L1}
\end{equation}

Une application de cette approche à la reconstruction de sources multiples est présenté dans \cite{Cheng2008}, où la régularisation $L_1$ est justifiée par la nature parcimonieuse du vecteur d'état à estimer. Une autre étude, menée dans \cite{Martinez2013} porte sur la modélisation des rejets de xénon durant l'incident de Fukushima sous la forme de brèves impulsions (ou \textit{short bursts}), fournissant ainsi un cadre approprié à la régularisation $L_1$.

\subsubsection{Une vision probabiliste du problème d'optimisation}

Il est possible d'établir une analogie entre les méthodes d'optimisation précédemment mentionnées et les approches se basant sur des calculs de lois de probabilité.

En effet, dans un cadre probabiliste, {l'optimisation sans terme régularisant est équivalente à la résolution d'un} problème de \textit{maximum de vraisemblance}. En pratique, le fait de minimiser la fonction-coût $\CostF_2$ revient ainsi à déterminer la grandeur suivante:

\begin{equation}
{
\widehat{\VecSigma_2} = \argmax_\VecSigma~ \log p(\VecObs | \VecSigma)
\label{eq_ML_MCO}
}
\end{equation}
où $p(\VecObs | \VecSigma)$ est la vraisemblance des observations pour un terme source donné. \\

De même, le fait de régulariser la fonction-coût par l'ajout d'un terme d'ébauche revient à adopter une démarche \textit{bayésienne}\footnote{Nous décrirons plus en détail les principes fondamentaux de la statistique bayésienne dans le prochain chapitre.}, où le terme régularisant représente une information a priori $p(\VecSigma)$, et l'estimateur recherché devient alors le maximum de la loi a posteriori de $\VecSigma$: 

\begin{equation}
{
\begin{split}
\widehat{\VecSigma_{2_B}} &= \argmax_\VecSigma~ \log p(\VecSigma | \VecObs) \\
 &= \argmax_ \VecSigma \left(\log p(\VecObs | \VecSigma) + \log p(\VecSigma)\right)
\label{eq_MAP}
\end{split}
}
\end{equation}

{
Dans le cas de la régularisation $L_1$ présenté à l'équation \eqref{eq_MCO_L1}, la démarche est équivalente à l'introduction d'une information a priori de parcimonie sous la forme d'une loi laplacienne pour $p(\VecSigma)$. Cette démarche de maximum a posteriori est alors équivalente à l'estimateur \textit{Least Absolute Shrinkage and Selection Operator} (LASSO) \cite{Tibshirani1996}.\\
}
\subsection{Formulation générale et méthodes de résolution}

Le fait de présenter le problème inverse de la reconstruction du terme source sous une forme linéaire permet sa résolution directe grâce aux méthodes précédemment présentées. Cependant, le fait de dépendre d'un maillage sur le temps et l'espace peut poser problème, en particulier dans les cas où la finesse de ce maillage ramène l'estimation du terme source à un problème de grande dimension, qui devient alors difficile à résoudre. \\

Il reste toutefois possible d'écrire le problème direct sous une forme plus générale, qui ne tient pas compte d'un maillage et qui reste valable pour le cas multi-source. Dans un tel contexte, la relation source-récepteur se traduit par l'équation suivante:

\begin{equation}
	\VecObs = \sum\limits_{i=1}^{N_s} \MatC(\PosSource^{(i)})\VecQSource^{(i)} + \VecErreur
	\label{eq_SR_nonlineaire}
\end{equation}
où :
\begin{itemize}
	\item $N_s$ est le nombre de sources, 
	\item $\PosSource^{(i)}$ et $\VecQSource^{(i)}$ sont respectivement la position et le profil temporel d'émission de la $i$-ème source,
	\item $\MatC(\PosSource^{(i)})$ est la concentration résultant d'un rejet unitaire de la $i$-ème source.\\
\end{itemize}

{Ici, la relation source-récepteur ne suppose désormais qu'une discrétisation temporelle sur les $T_s$ dimensions du vecteur $\VecQSource$, contrairement à un cas maillé où  la dimension du problème est de $N_xN_yN_zT_s$.} Toutefois, {il devient impossible d'obtenir directement les paramètres $\PosSource$ et $\VecQSource$ de la source, contrairement à l'équation \eqref{eq_relation_SR_non_parametrique} où $\VecSigma$ peut se calculer analytiquement grâce à la nature linéaire du système à résoudre. En conséquence, d'autres méthodes plus appropriées sont nécessaires pour résoudre le problème inverse.} 

\subsubsection{Algorithmes évolutionnaires}

Le gain en complexité du problème d'optimisation à résoudre {avec} la formulation non-linéaire nécessite une approche suffisamment robuste afin de réussir à traiter les fonctions-coût résultantes. Cela est possible par le biais de \textit{métaheuristiques} (méthodes d'optimisation généralement appliquées des problèmes à forte complexité combinatoire). Parmi celles-ci, les \textit{approches évolutionnaires}, et en particulier les \textit{algorithmes génétiques} ont été utilisés à plusieurs reprises pour résoudre des problèmes STE.\\
Les algorithmes évolutionnaires s'inspirent du principe de l'évolution darwinienne des populations biologiques, et leur construction s'appuie sur le fait que l'apparition d'espèces adaptées au milieu est la conséquence de la conjonction de deux phénomènes distincts : 

 \begin{itemize}
 	\item d'une part, la \textit{sélection naturelle} imposée par le milieu (les individus les plus adaptés survivent et se reproduisent),
 	\item d'autre part, les variations non-contrôlées du matériel génétique des espèces.
 \end{itemize}
 
 Pour un problème d'optimisation standard, la fonction-coût $\CostF$ à minimiser devient, dans le langage évolutionnaire, une \textit{fonction d'adaptation}. Les points du domaine $\Omega$ des paramètres à explorer sont appelés \textit{individus}, et l'ensemble d'individus est appelé \textit{population}. \\
 La structure d'un algorithme évolutionnaire se compose de plusieurs étapes. Dans un premier temps, on initialise une population $\Pi_0$ en tirant $p$ individus dans $\Omega$ de façon uniformément aléatoire, et on l'évalue en calculant les valeurs de $\CostF$ pour chaque individu. Vient ensuite une boucle itérative qui implémente le processus de sélection naturelle:
 \begin{enumerate}
 	\item on sélectionne les individus les plus performants (au sens de $\CostF$) de la population $\Pi_i$ à l'itération courante $i$, que l'on appelle \textit{parents},
 	\item on applique des opérateurs de variation aux parents sélectionnés, pour générer de nouveaux individus: les \textit{enfants}. On parle de mutation si l'opérateur est 1-aire (i.e. ne prend qu'un seul individu en argument) et de croisement si l'opérateur est $n$-aire (i.e. prend $n$ individus en argument, avec $n \geq 2$). Notons que cette étape est purement stochastique, et que les opérateurs n'utilisent pas d'information sur la performance des précédentes générations: on parle alors d'opérateurs semi-aveugles.
 	\item on évalue les enfants avec $\CostF$,
 	\item on remplace $\Pi_i$  par une nouvelle population $\Pi_{i+1}$ créée à partir des enfants et des parents de $\Pi_i$ au moyen d'une sélection darwinienne.\\
 \end{enumerate}
 
 Dans notre contexte, on peut assimiler $\Omega$ à l'espace des paramètres du terme source. Historiquement, les premières méthodes d'estimation basées sur les algorithmes génétiques (voir par exemple \cite{Haupt2005}) supposaient connues certaines informations a priori comme les emplacements potentiels de la source. 
 
 L'utilisation des algorithmes génétiques pour la résolution des problèmes STE peut être illustré à travers plusieurs exemples: \\
 
 \begin{itemize}
 	\item Dans \cite{Allen2007} et \cite{Haupt2007}, il est envisagé de se placer dans une situation plus réaliste où les paramètres à estimer comprennent non seulement la position de la source et la masse totale émise, mais également la vitesse et la direction du vent. \cite{Cervone2011} proposent une extension du processus de variation en couplant les opérateurs de croisement et de mutation avec un algorithme non-darwinien d'inférence statistique.
 \item L'approche génétique a également permis d'étudier le rapport entre la précision du terme source reconstruit et le nombre de capteurs utilisés: \cite{Long2010} reprend ce problème de précision en testant un algorithme génétique sur différentes configurations de réseaux de capteurs plus ou moins denses, et en tenant compte du caractère bruité des données fournies par les détecteurs. 
 \item Plus récemment, la méthodologie a été validée sur des données expérimentales de rejet multi-sources, comme présenté dans les travaux de \cite{Cantelli2015} où jusqu'à trois sources distinctes ont pu être caractérisées.\\
 \end{itemize}
 
 \subsubsection{Méthodes bayésiennes par simulation stochastique}
 
 On a vu qu'il est possible de voir l'estimation du terme source sous un angle bayésien  comme étant la recherche du maximum  de la loi a posteriori de ses paramètres. Au lieu de se limiter à une estimation ponctuelle, une meilleure alternative consiste à calculer l'ensemble de cette distribution a posteriori, afin de pouvoir bénéficier {de l'ensemble de l'information statistique} disponible sur les paramètres à estimer. 
 
 L'étude menée par \cite{Sohn2002} se penche sur le cas particulier de l'estimation d'une source à l'intérieur d'un bâtiment. Pour cela, un calcul initial simule un nombre $N$ fixé de scénarios $S_1,\dots,S_N$ possibles à partir d'un échantillon de paramètres $\VecTheta_1, \dots, \VecTheta_N$ potentiels de la source. Chaque scénario $S_k$ représente un jeu de mesures simulées issues d'une source de paramètres $\VecTheta_k$. Une fois cette collection constituée, la loi a posteriori suivante est calculée via la règle de Bayes, pour chaque scénario $S_k$:
 
 \begin{equation}
 \label{eq_bayes_monte_carlo}
 p(S_k|\VecObs) = \dfrac{p(S_k)p(\VecObs | S_k)}{p(\VecObs)}
 \end{equation}
 
 Cette première approche permet de caractériser un rejet, {et nuance les performances de l'algorithme d'estimation en fonction du mode d'acquisition des mesures de concentrations. La comparaison est faite entre des observations acquises simultanément depuis tous les capteurs, et des observations recueillies de façon séquentielle, capteur par capteur.}
 
 Toutefois, l'expression analytique de la  loi a posteriori des paramètres de la source n'est  pas accessible de façon directe: du fait de la formulation présentée à l'équation \eqref{eq_SR_nonlineaire}, elle nécessite un calcul du modèle de dispersion à chaque fois qu'un jeu de paramètres doit être évalué. L'exploration systématique de l'espace des paramètres peut alors se révéler très coûteuse en temps de calcul. Il devient alors nécessaire d'introduire des méthodes numériques afin d'explorer de façon optimale cet espace: c'est l'objet des techniques de \textit{simulation stochastique}, aussi appelées \textit{méthodes de Monte-Carlo}.

 Parmi ces algorithmes, la catégorie la plus connue est certainement celle des méthodes  \textit{Markov Chain Monte-Carlo} (MCMC). Dans \cite{Keats2007} et \cite{Chow2008}, la méthode MCMC est employée dans un contexte urbain, et couplée à des modèles de dispersion suffisamment performants pour une prise en compte du milieu bâti. \cite{Senocak2008} propose l'utilisation d'un modèle de dispersion plus simple de type gaussien, amélioré par l'ajout de paramètres stochastiques relatifs à la diffusion turbulente, eux-mêmes estimés par l'algorithme MCMC en plus des paramètres de la source. \cite{Yee2008b} étend la méthodologie aux cas où le nombre de sources est inconnu et considéré comme un paramètre supplémentaire à estimer, ajoutant ainsi une étape de sélection de modèle dans la procédure d'estimation. Cela est possible grâce à une méthode MCMC à \textit{sauts réversibles}, qui associe pour chaque nombre de sources possible un espace de paramètres différent à explorer. Enfin, \cite{Yee2014} propose une extension des concepts de \cite{Keats2007} à l'échelle globale, où un algorithme MCMC est utilisé pour reconstruire la position et le profil d'émission d'une usine d'isotopes médicaux à partir des mesures de xénon relevées par le réseau mondial des capteurs de l'OTICE\footnote{L'\textit{Organisation du Traité d'Interdiction Complète des Essais Nucléaires} (OTICE, ou CTBTO en anglais: \textit{Comprehensive Nuclear-Test Ban Treaty Organization)} a pour rôle de détecter, grâce à un réseau de capteurs déployés sur l'ensemble du globe, et de signaler à tous les pays signataires du traité toute explosion d'origine atomique, pour prendre des mesures afin d'empêcher les puissances nucléaires actuelles de poursuivre leurs essais, et les Etats ne disposant pas de l'arme atomique de s'en doter.}. \\

\section{Problématique de recherche}

Le développement des méthodes STE dans le contexte de la dispersion atmosphérique est une branche scientifique relativement récente par rapport au cadre général de la physique de l'atmosphère. Il s'agit également d'un domaine de recherche largement pluridisciplinaire, mêlant des compétences en physique mais également sur des aspects mathématiques (optimisation, statistiques, simulation aléatoire...) et informatiques (algorithmique, calcul scientifique haute performance...) variés. \\

Même si de nombreux aspects sont couverts par les éléments présents dans l'état de l'art exposé en paragraphe §\ref{section_etat_art_STE}, de nombreuses questions restent encore ouvertes. 
Dans le cas particulier de situations accidentelles, il est ainsi important pour les primo-intervenants de disposer d'une information fiable avec une certaine quantification de l'incertitude, à partir d'un nombre potentiellement faible de mesures, et dans un intervalle de temps raisonnablement court pour assurer l'efficacité de l'intervention. Les questions de vitesse de calcul sont alors, de fait, importantes: si elles ne posent pas de problèmes dans le cadre d'une étude \textit{a posteriori}, il en va autrement en situation de crise, où le temps de réponse à un incident est un paramètre primordial. {Dans l'inventaire des méthodes existantes, les approches basées sur l'optimisation  d'une fonction-coût fournissent une estimation déterministe, dont l'interprétation en termes d'évaluation de l'incertitude est plus difficile qu'avec des méthodes probabilistes. Les méthodes bayésiennes stochastiques ne sont pas non plus exemptes de défauts, à l'image de l'application de l'algorithme MCMC dans les travaux de \cite{Chow2008}, où les temps de calcul sont bien trop importants pour une application en situation opérationnelle. Ce cas d'étude illustre ainsi le fait que l'approche bayésienne peut rapidement se révéler coûteuse si l'algorithme choisi n'est pas suffisamment élaboré pour tenir la charge de calcul nécessaire au processus d'estimation.} 

Un autre aspect important est celui de la nature de la source à retrouver. Pour le contexte accidentel, si on se place dans le cas d'un attentat à la bombe sale ou dans celui d'une fuite sur un complexe industriel, l'hypothèse d'une source localisée est raisonnable. Toutefois il peut être intéressant de se pencher plus en détail sur le profil du rejet, celui-ci n'étant pas forcément instantané. {Plusieurs techniques existantes introduisent des hypothèses sur le type de source recherché, notamment par rapport à son profil d'émission: celui-ci est le plus souvent considéré comme constant. Dans les cas où le rejet est potentiellement plus complexe, on a souvent l'information sur la position réelle de la source ou du moins sur un ensemble d'emplacements potentiels. Il n'existe pas à ce jour de cadre méthodologique combinant une localisation de la source sans a priori sur sa position et une estimation de son profil d'émission pouvant varier dans le temps.}\\

\textit{Les travaux présentés dans ce manuscrit et ayant fait l'objet du travail de thèse ont donc pour but: }

\begin{itemize}
	\item \textit{de développer et valider une méthode basée sur l'inférence bayésienne, capable de caractériser un point source par sa localisation et son profil de rejet à partir de mesures issues d'un réseau de capteurs, }
	\item \textit{de coupler cette méthode avec un modèle de dispersion atmosphérique au sein d'une chaîne de calcul pouvant, à terme, être utilisée en situation opérationnelle.} \\
\end{itemize}


Pour cela, nous {aurons} recours à une approche bayésienne stochastique différente des algorithmes MCMC de par sa philosophie, et basée sur un principe d'échantillonnage d'importance adaptatif. De telles méthodes permettent en effet une convergence suffisamment rapide, et pallient certaines difficultés rencontrées par les MCMC. Nous associons cette méthode adaptative, utilisée pour localiser la source, à un calcul analytique du profil d'émission, permis par le choix d'une hypothèse de gaussianité sur le vecteur $\VecSigma$, et accompagné par l'implémentation d'une étape de contrainte visant à assurer la positivité de $\VecSigma$. Le schéma d'implémentation que nous avons conçu permet ainsi de produire une estimation des paramètres de position et d'émission d'une source unique via un schéma itératif de couplage avec un modèle de dispersion, la procédure d'estimation à proprement parler demeurant indépendante du modèle choisi. \\

Pour développer cette thèse, le présent manuscrit est organisé de la façon suivante. Après avoir présenté la problématique posée par le sujet et effectué un premier tour d'horizon de la thématique dans ce chapitre introductif, nous développons au Chapitre 2 les principes régissant l'inférence bayésienne, en nous concentrant plus particulièrement sur les applications des méthodes de Monte-Carlo en statistique bayésienne. Nous y détaillons le cheminement théorique permettant la construction de l'algorithme AMIS, qui constitue un élément central des travaux de cette thèse. 
Dans le Chapitre 3, nous présentons l'application de cet algorithme adaptatif à la question de la reconstruction du terme source en dispersion atmosphérique. Pour cela, nous exploitons un cas d'application pratique issu d'une campagne de mesures expérimentales dont nous expliquons les conditions de réalisation. Le Chapitre 4 détaille une variante de la méthodologie présentée au Chapitre 3, avec l'utilisation d'une approche orientée récepteur, et l'emploi d'un code de dispersion de type lagrangien dans des configurations simulées de {milieu naturel} et de milieu urbain.
{Enfin, la dernière partie de ce manuscrit présentera une conclusion générale sur les résultats présentés, et introduira les différentes perspectives que ceux-ci ont permis d'ouvrir.}