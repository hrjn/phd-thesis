
	\subsection{Paramétrisation et relation source-récepteur}
	
	La façon la plus générale de formuler mathématiquement les caractéristiques d'un terme source consiste à définir un champ complet d'émission sur 4 dimensions (espace et temps), chaque élément de ce champ représentant une émission à un point et à un instant donnés. Si l'espace du domaine considéré est discrétisé en $N_xN_yN_z$ points, chaque point étant associé à un profil d'émission de longueur $T_s$, alors le terme source est défini comme un vecteur $\VecSigma \in \mathbb{R}^{N_\VecSigma}$ (où $N_\VecSigma = N_xN_yN_zT_s$).Il est ainsi possible de lier les observations générées par le modèle et le vecteur source $\VecSigma$ par la formule suivante, appelée \textit{relation source-récepteur}:
	
	
	\begin{equation}
		\label{eq_relation_SR_non_parametrique}
		\VecObs = \bm{H}\VecSigma + \bm{\varepsilon}
	\end{equation}
	
	où $\VecObs \in \mathbb{R}^m$ représente le vecteur des observations, $\VecSigma \in \mathbb{R}^{N_\VecSigma}$ la source discrétisée et $\bm{H} \in \mathbb{R}^{m \times N_\VecSigma}$  la \textit{matrice de transfert}. Le rôle de $\bm{H}$ est d'assurer la projection depuis l'espace $\mathbb{R}^{N_\VecSigma}$  de la source vers l'espace $\mathbb{R}^m$ des observations grâce à l'utilisation d'un modèle de dispersion atmosphérique. $\bm{\varepsilon} \in \mathbb{R}^m$ est le vecteur des erreurs d'observation, et prend en compte les erreurs de mesure, de représentativité et de modèle. \\
	
	Dans le cas particulier d'une source ponctuelle on peut, dans l'équation \eqref{eq_relation_SR_non_parametrique}, substituer $\VecSigma$ par un profil d'émission $\VecSigma$ issu d'un unique point du domaine spatial considéré. En procédant ainsi, il n'est plus nécessaire de définir arbitrairement un maillage conditionné par $N_x, N_y, N_z$  sur le domaine. De plus, la dimension du problème se réduit à estimer un point de l'espace et son profil temporel associé, soit $T_s + 3$ inconnues, contrairement au cas général où il faut retrouver l'intégralité du champ d'émission, soit $N_xN_yN_zT_s$ inconnues.\\
	
		
	\section{Estimation du terme source: un état de l'art}
	\label{section_etat_art_STE}
	
	\subsection{Une brève introduction aux problèmes inverses}
	Le domaine des problèmes inverses constitue un vaste champ de la littérature scientifique, et de nombreux travaux y ont été menés. En pratique, résoudre un problème inverse revient à reconstituer un signal, une image, ou plus généralement une donnée non-observable à partir de mesures existantes, appelées observations.
	Cette approche est duale à celle du problème direct où, à partir d'un signal ou d'un ensemble de paramètres initiaux, on cherche à en calculer les effets après une transformation donnée (figure \ref{fig_diagramme_direct_inverse}).
	De nombreux domaines théoriques et pratiques font appel aux méthodes inverses. Le lecteur pourra trouver une revue complète de ces applications dans \cite{Tarantola2004}, on peut en citer quelques exemples tels que la géophysique \cite{Backus1967}, l'acoustique \cite{Kirsch1988}, l'imagerie satellite \cite{Park2003} et médicale \cite{Arridge1999}, les transferts thermiques \cite{McCormik1992} ou encore la finance quantitative \cite{Dembo1999}.\\
	
	\begin{figure}
		\centering
		\includegraphics[scale=0.5]{diagramme_direct_inverse.png}
		\caption{Schéma de principe illustrant la dualité entre problèmes direct et inverse.}
		\label{fig_diagramme_direct_inverse}
	\end{figure}
	
	
	En dispersion atmosphérique, le problème direct peut ainsi se traduire par la donnée des paramètres de la source au modèle de dispersion, qui va produire le champ de concentration résultant. Cela revient bien à définir le problème inverse comme étant la reconstruction des paramètres de la source à partir des mesures de concentrations aux capteurs. Nous décrivons dans les paragraphes suivants les différentes approches existantes dans la littérature permettant de résoudre ce problème. Des éléments complémentaires sur la question sont également disponibles dans \cite{Rao2007}. Dans un souci de brièveté, par la suite nous qualifierons de "problèmes STE"(\textit{source term estimation) toutes les études de cas visant à estimer les paramètres d'une ou de plusieurs sources inconnues}.\\
	
	
 \subsection{Backtracking et modèles adjoints}
 
 Une première possibilité consiste à étudier le problème dual associé à l'équation d'advection-diffusion  \eqref{eqn_advection_diffusion}, en inversant la flèche du temps, et par conséquent la direction du vent.  Par le principe de symétrie présenté dans \cite{Hourdin2006a}, on peut alors introduire la notion de \textit{rétro-transport} ou \textit{backtracking} en  réécrivant l'équation d'advection-diffusion sous la forme : 
 
 \begin{equation}
 \label{eqn_advection_diffusion_backward}
 \dfrac{-\partial C^*}{\partial t} - \nabla \cdot(C^*\bm{\vec{u}}) = \nabla \cdot (\bm{K}\nabla C^*) + \VecObs
 \end{equation}
 
 où $C^*$ est un champ de concentrations conjuguées, ou \textit{rétro-concentrations}, dont les valeurs sont obtenues par des rétro-rejets virtuels depuis les capteurs vers la source. Le modèle de dispersion associé à cette démarche duale est alors appelé \textit{modèle adjoint}, ou \textit{modèle backward}. Cette démarche revient ainsi à transformer le champ d'émission de \eqref{eq_relation_SR_non_parametrique} en un champ de rétro-émissions issues des capteurs, comme illustré sur les figures \ref{schema_probleme_direct} et \ref{schema_probleme_inverse}, et ainsi fournir une estimation qualitative du terme source.\\
 
 \begin{figure}[h]
 	\begin{subfigure}{0.5\textwidth}
 		\includegraphics[scale=0.6]{schema_probleme_direct.png}
 		\caption{Approche orientée source}
 		\label{schema_probleme_direct}
 	\end{subfigure}
 	\begin{subfigure}{0.5\textwidth}
 		\includegraphics[scale=0.2]{schema_probleme_inverse.png}
 		\caption{Approche orientée récepteur}
 		\label{schema_probleme_inverse}
 	\end{subfigure}
 	\caption{Exemple illustrant la relation entre une source unique $S$ et trois capteurs $R_1,R_2,R_3$ sur un problème en deux dimensions pour les approches en "direct" (\ref{schema_probleme_direct}) et en "adjoint" (\ref{schema_probleme_inverse}). }
 \end{figure}
 
 Dans \cite{Flesch1995}, un modèle \textit{backward} lagrangien est construit pour estimer le profil de rejet d'une source surfacique.  Cette méthode est également appliquée dans \cite{Pudykiewicz1998}, où la position et l'intensité du terme source de Tchernobyl sont reconstruits. De même, dans \cite{Hourdin2006b} le principe du \textit{backtracking} est appliqué à un modèle eulérien pour retrouver la source de la campagne ETEX\footnote{L’expérience \textit{European Tracer EXperiment} (ETEX), menée en 1997,  a consisté à mesurer et étudier l'impact à l'échelle européenne de rejets de gaz traceurs passifs émis depuis le nord de la France \cite{Nodop1998}.}.\\
 
 \subsection{Optimisation et méthode des moindres carrés}
 \label{subsection_MCO}
 
 L'équation \eqref{eq_relation_SR_non_parametrique} peut être réécrite sous la forme : 
 
 \begin{equation}
 \VecObs = \widehat{\VecObs} + \VecErreur
 \label{eq_relation_SR_nonparam_simple}
 \end{equation}

où $\widehat{\VecObs} = \MatH \VecSigma$ est l'approximation des observations $\VecObs$ par le processus de modélisation. On voit bien que dans le cas parfait où ce dernier reproduit exactement les observations attendues, on a $\VecErreur = 0$ et par suite $\widehat{\VecObs} = \VecObs$. Le fait de chercher la valeur $\VecSigma$ pour laquelle $\widehat{\VecObs}$ se rapproche le plus de $\VecObs$ permet ainsi de définir le problème STE sous la forme d'une minimisation de l'erreur $\VecErreur$: on définit pour cela une fonction-coût qui s'écrit : 

\begin{equation}
\CostF(\VecSigma) = ||\VecObs - \MatH \VecSigma||_p = ||\VecObs- \widehat{\VecObs}||_p
\label{eq_cost_function_definition}
\end{equation}
où $||\cdot||_p$ désigne une norme $L_p$ arbitrairement choisie. La résolution du problème STE se traduit alors par la minimisation de cette fonction-coût:

\begin{equation}
\widehat{\VecSigma} = \argmin_\VecSigma \mathcal{J}(\VecSigma)
\label{eq_argmin_cost_function}
\end{equation}
où $\widehat{\VecSigma}$ est l'estimation du terme source recherchée. Il existe différentes approches  pour aborder ce problème d'optimisation, qui est ici posé dans un cas générique. Dans ce paragraphe, nous étudions une des méthodes les plus courantes pour résoudre l'équation \eqref{eq_argmin_cost_function}, à savoir la méthode des \textit{moindres carrés ordinaires} (MCO). Celle-ci consiste à choisir une norme $L_2$ pour la définition de la fonction-coût, ainsi qu'un écart quadratique entre $\widehat{\VecObs}$ et $\VecObs$ : 

\begin{equation}
\CostF_{2}(\VecSigma) = ||\VecObs - \MatH \VecSigma ||_2^2
\label{eq_cost_MCO}
\end{equation}

L'erreur que nous cherchons à minimiser est une combinaison des différentes sources d'erreur existantes (mesure, représentativité, modèle). Comme il est difficile de quantifier les contributions respectives de chacune de ces erreurs, on peut invoquer le principe du maximum d'entropie pour définir la représentation statistique de l'ensemble de ces erreurs comme étant un bruit gaussien centré et de matrice de covariance $\MatR$ : 

\begin{equation}
\VecErreur \sim \mathcal{N}(0,\MatR)
\label{eq_bruit_obs}
\end{equation}

La fonction-coût peut alors s'écrire sous la forme suivante \cite{Winiarek2011}:

\begin{equation}
\mathcal{J}_2(\VecSigma) = \dfrac{1}{2}(\VecObs - \MatH \VecSigma)^T \MatR^{-1}(\VecObs - \MatH\VecSigma)
\label{eq_cost_forme2}
\end{equation}

Sous cette hypothèse, minimiser $\CostF_2$ revient de façon équivalente à calculer l'estimateur du \textit{maximum de vraisemblance} (EMV), i.e. à maximiser la quantité $p(\VecObs | \VecSigma)$: 

\begin{equation}
\widehat{\VecSigma_2} = \argmax_\VecSigma~ p(\VecObs | \VecSigma)
\label{eq_ML_MCO}
\end{equation}

Dans \cite{Kathirgamanathan2002}, un point source instantané est estimé à l'aide d'une méthode des moindres carrés, cette source étant caractérisée par sa masse totale émise, sa position et son instant d'émission. L'étude qui y est menée met notamment l'accent sur l'influence du dimensionnement du réseau de capteurs sur la performance de la reconstruction : au moins trois points de mesure sont nécessaires, et la distance entre les différents capteurs influe sur la qualité de l'estimation.\\

Dans \cite{Ryall2001}, ce sont des zones d'émission plus grandes qui sont estimées pour des rejets de gaz à effet de serre sur plusieurs années. \\

Dans \cite{Matthes2005}, une approche en deux temps est formulée afin de résoudre un problème de complexité proportionnellement croissante au nombre de capteurs. Dans une première étape, les mesures individuelles de chaque capteur produisent des ensembles de positions probables pour la source. Dans un deuxième temps, ces ensembles sont comparés pour estimer la meilleure position en faisant varier l'intensité de la source à retrouver. \\

 Dans \cite{Robertson1998}, l'inversion est faite dans le cadre de l'assimilation de données: cette méthodologie permet de corriger de façon variationnelle les paramètres du modèle de dispersion selon une boucle de rétro-action basée sur des observations. Très utilisée en météorologie, l'assimilation de données agit en deux temps selon un principe de "prévision-correction", et itère sur le cycle suivant:
 \begin{enumerate}
 	\item  on calcule d'abord une ébauche de l'état à l'instant présent $t$, en appliquant un modèle de prévision en $t-1$,
 	\item  on exploite ensuite les observations disponibles à l'instant $t$ pour corriger les paramètres du modèle en les comparant avec l'ébauche. \\
 \end{enumerate}
 
 \cite{Issartel2005} mentionne le principe d'\textit{illumination}, qui est une mesure quantitative de la représentativité des mesures dans le temps et l'espace. Dans le cadre d'un modèle adjoint, l'illumination permet ainsi de caractériser des zones qui ne sont pas forcément couvertes par les rétro-émissions issues des capteurs, par exemple si la zone considérée est relativement éloignée du point de mesure le plus proche. Cependant, si l'information d'illumination est utilisée pour inverser un terme source, alors elle aura tendance à fortement favoriser les solutions proches des capteurs. Pour compenser ce problème, une phase de \textit{renormalisation} est introduite (\cite{Issartel2007} et \cite{Sharan2009}) pour équilibrer l'information apportée par chacun des capteurs. D'abord utilisée à grande échelle, cette méthodologie a récemment été validée à l'échelle locale dans \cite{Singh2014}, et dans un contexte urbain par \cite{Kumar2015}. \\
 
 
 \subsection{Algorithmes évolutionnaires}
 
 Le problème d'optimisation posé par l'équation \eqref{eq_argmin_cost_function} peut également se résoudre par le biais de \textit{métaheuristiques} (méthodes d'optimisation généralement appliquées des problèmes à forte complexité combinatoire). Parmi celles-ci, les \textit{approches évolutionnaires}, et en particulier les \textit{algorithmes génétiques} ont été utilisés à plusieurs reprises pour résoudre des problèmes STE.\\
 
Les algorithmes évolutionnaires s'inspirent du principe de l'évolution darwinienne des populations biologiques, et leur construction s'appuie sur le fait que l'apparition d'espèces adaptées au milieu est la conséquence de la conjonction de deux phénomènes distincts : 

 \begin{itemize}
 	\item d'une part, la \textit{sélection naturelle} imposée par le milieu (les individus les plus adaptés survivent et se reproduisent),
 	\item d'autre part, les variations non-contrôlées du matériel génétique des espèces.
 \end{itemize}
 
 Pour un problème d'optimisation standard, la fonction-coût $\CostF$ à minimiser devient, dans le langage évolutionnaire, une \textit{fonction d'adaptation}. Les points du domaine $\Omega$ des paramètres à explorer sont appelés \textit{individus}, et l'ensemble d'individus est appelé \textit{population}. \\
 
 La structure d'un algorithme évolutionnaire se compose de plusieurs étapes. Dans un premier temps, on initialise une population $\Pi_0$ en tirant $p$ individus dans $\Omega$ de façon uniformément aléatoire, et on l'évalue en calculant les valeurs de $\CostF$ pour chaque individu. Vient ensuite une boucle itérative qui implémente le processus de sélection naturelle:
 \begin{enumerate}
 	\item on sélectionne les individus les plus performants (au sens de $\CostF$) de la population $\Pi_i$ à l'itération courante $i$, que l'on appelle \textit{parents},
 	\item on applique des opérateurs de variation aux parents sélectionnés, pour générer de nouveaux individus: les \textit{enfants}. On parle de mutation si l'opérateur est 1-aire (i.e. ne prend qu'un seul individu en argument) et de croisement si l'opérateur est $n$-aire (i.e. prend $n$ individus en argument, avec $n \geq 2$). Notons que cette étape est purement stochastique, et que les opérateurs n'utilisent pas d'information sur la performance des précédentes générations: on parle alors d'opérateurs semi-aveugles.
 	\item on évalue les enfants avec $\CostF$,
 	\item on remplace $\Pi_i$  par une nouvelle population $\Pi_{i+1}$ créée à partir des enfants et des parents de $\Pi_i$ au moyen d'une sélection darwinienne.\\
 \end{enumerate}
 
 Dans notre contexte, on peut assimiler $\Omega$ à l'espace des paramètres du terme source. Historiquement, les premières méthodes d'estimation basées sur les algorithmes génétiques (voir par exemple \cite{Haupt2005}) supposaient connues certaines informations a priori comme les emplacements potentiels de la source. Dans \cite{Allen2007} et \cite{Haupt2007}, il est envisagé de se placer dans une situation plus réaliste où les paramètres à estimer comprennent non seulement la position de la source et la masse totale émise, mais également la vitesse et la direction du vent. \cite{Cervone2011} propose une extension du processus de variation en couplant les opérateurs de croisement et de mutation avec un algorithme non-darwinien d'inférence statistique.\\ 
 
 Une telle approche a également permis d'étudier le rapport entre la précision du terme source reconstruit et le nombre de capteurs utilisés. \cite{Long2010} reprend ce problème de précision en testant un algorithme génétique sur différentes configurations de réseaux de capteurs plus ou moins denses, et en prenant en compte le caractère bruité des données fournies par les détecteurs. \\
 
 L'approche génétique a récemment été validée sur des données expérimentales de rejet multi-sources, comme présenté dans les travaux de \cite{Cantelli2015} où jusqu'à trois sources distinctes ont pu être caractérisées.\\
 
 \subsection{Méthodes bayésiennes déterministes}
 
 Plusieurs travaux de la littérature STE privilégient une approche \textit{bayésienne}\footnote{Nous décrirons plus en détail les principes fondamentaux de la statistique bayésienne dans le prochain chapitre.} à celle du maximum de vraisemblance présentée au paragraphe §\ref{subsection_MCO}. Concrètement, ces travaux considèrent la distribution a posteriori $p(\VecSigma|\VecObs)$ du terme source comme étant la grandeur d'intérêt, celle-ci pouvant s'exprimer en fonction de la vraisemblance $p(\VecObs | \VecSigma)$ et de l'information a priori $p(\VecSigma)$ disponible sur $\VecSigma$ grâce à la règle de Bayes : 
 
 \begin{equation}
 p(\VecSigma|\VecObs) = \dfrac{p(\VecObs | \VecSigma)p(\VecSigma)}{p(\VecObs)}
 \label{eq_regle_bayes_edl}
 \end{equation}
 
Le fait d'utiliser un a priori sur $\VecSigma$ dans le cadre bayésien revient, dans un cadre d'optimisation, à introduire une \textit{ébauche} du terme source, parfois appelée \textit{first-guess} ou \textit{background}, que nous noterons $\VecSigmaB$

\subsubsection{Régularisation $L_2$}

On met ici l'équation \eqref{eq_cost_MCO} sous la forme: 
 
 \begin{equation}
 \mathcal{J}_{2_B}(\VecSigma) = ||\VecObs - \MatH \VecSigma||_2^2 + \lambda_B ||\VecSigmaB||_2
 \label{eq_cost_MCO_reg}
 \end{equation}
 
 L'équation \eqref{eq_cost_MCO_reg} illustre le principe de la régularisation de Tikhonov \cite{Tikhonov1963}, qui permet de traiter des problèmes inverses mal-posés\footnote{Un problème bien-posé au sens de Hadamard \cite{Hadamard1902} désigne un modèle mathématique dont la solution existe, est unique, et dépend de façon continue des données. A l'inverse, un problème mal-posé déroge à au moins une des règles précédemment citées. De nombreux problèmes physiques sont mal-posés et nécessitent d'être \textit{régularisés} afin de pouvoir être traités.} du fait du faible nombre d'observations et de la faible observabilité potentielle de la source, qui engendrent une infinité de solutions. L'ajout du terme régularisant $\lambda_B ||\VecSigmaB||_2$ permet ainsi de garantir l'unicité de la solution du problème inverse. \\
 
 Dans le cas des statistiques gaussiennes (équation \eqref{eq_bruit_obs}), l'erreur d'ébauche $\VecSigma - \VecSigmaB$ suit également une loi normale centrée, de matrice de covariance $\MatB$:
 
 \begin{equation}
 (\VecSigma - \VecSigmaB) \sim \mathcal{N}(0,\MatB)
 \label{eq_erreur_ebauche}
 \end{equation}
 
 L'équation \eqref{eq_cost_forme2} devient alors \cite{Winiarek2011}:

\begin{equation}
\mathcal{J}_2(\VecSigma) = \dfrac{1}{2}(\VecObs - \MatH \VecSigma)^T \MatR^{-1}(\VecObs - \MatH\VecSigma) + \dfrac{1}{2}(\VecSigma - \VecSigmaB)^T\MatB^{-1}(\VecSigma - \VecSigma)
\label{eq_cost_reg_forme2}
\end{equation}
Le terme de droite fait ici office de régularisant. \\

Dans \cite{Winiarek2012}, un estimateur ponctuel de type \textit{maximum a posteriori} (MAP) est utilisé pour résoudre un problème STE à l'échelle continentale, cet estimateur est obtenu en calculant le \textit{Best Linear Unbiased Estimator} (BLUE), qui est une méthode basée sur une descente de gradient. \\

Dans le même cadre, \cite{Saunier2013} propose une reconstruction du terme source de l'accident de Fukushima en deux temps. Tout d'abord, une première fonction-coût est minimisée pour estimer la période du rejet, puis tenant compte de cette contrainte, une seconde fonction-coût est optimisée pour retrouver la source à proprement parler. 
Il est à noter que l'équation \eqref{eq_cost_reg_forme2} correspond à celle de la méthode d'assimilation variationnelle de données 3D-var \cite{Courtier1998}. La vision de ce type d'estimation se rapproche ainsi de celle de l'assimilation de mesures issues de capteurs pour reconstruire un état inconnu, qui est ici le champ d'émission de la source recherchée.\\

Une variante pour cette approche consiste à se placer dans un cadre statistique non-gaussien, notamment si on cherche à caractériser l'ébauche suivant certaines hypothèses spécifiques \cite{Bocquet2005a}. Les auteurs de \cite{Krysta2007} font l'hypothèse d'une source ponctuelle et instantanée pour définir une forme particulière d'ébauche, et utilisent un principe de maximum d'entropie sur la moyenne \cite{Jaynes1957} pour réécrire la fonction-coût et en dériver un estimateur approprié pour $\VecSigma$.\\

\subsubsection{Régularisation $L_1$}

Il peut exister des situations où les observations prennent l'allure de \textit{signaux parcimonieux}, autrement dit elles sont décrites par un très faible nombre de valeurs non-nulles sur l'intervalle temporel de mesure. C'est par exemple le cas dans \cite{Martinez2013} où une étude sur les profils de rejets de xénon sous forme d'impulsions (ou \textit{short bursts}) est menée. Afin de prendre cela en compte, l'équation \eqref{eq_cost_MCO} est modifiée et c'est cette fois un terme régularisant sous une norme $L_1$ qui est utilisé: 

\begin{equation}
\mathcal{J}_{2_L} = ||\VecObs - \MatH\VecSigma||_2^2 + \lambda_L ||\VecSigma||_1
\label{eq_MCO_L1}
\end{equation}

Dans une optique bayésienne, cela revient à introduire une information a priori de parcimonie sous la forme d'une loi laplacienne sur $p(\VecSigma)$ et à calculer un estimateur de type LASSO \cite{Tibshirani1996}.


\subsection{Méthodes de simulation stochastique}

Afin d'avoir une solution au problème STE qui puisse être bien décrite du point de vue statistique, dans le cadre bayésien il est préférable d'estimer l'entière distribution a posteriori $p(\VecSigma | \VecObs)$ au lieu de se limiter à un estimateur ponctuel tel que le MAP décrit précédemment.\\


On peut prendre comme premier exemple l'étude menée par \cite{Sohn2002}, qui se penche sur le cas particulier de l'estimation de source à l'intérieur d'un bâtiment. Pour cela, un calcul initial simule un nombre $N$ fixé de scénarios $S_1,\dots,S_N$ possibles à partir d'un échantillon de paramètres $\VecTheta_1, \dots, \VecTheta_N$ potentiels de la source. Chaque scénario $S_k$ représente un jeu de mesures simulées issues d'une source de paramètres $\VecTheta_k$. Une fois cette collection constituée, la loi a posteriori suivante est calculée via la règle de Bayes, pour chaque scénario $S_k$:

\begin{equation}
\label{eq_bayes_monte_carlo}
p(S_k|\VecObs) = \dfrac{p(S_k)p(\VecObs | S_k)}{p(\VecObs)}
\end{equation}

Cette première approche, appelée \textit{Bayes Monte Carlo} (BMC), permet de caractériser un rejet mais peut également servir à placer de façon optimale les capteurs d'un réseau de mesure. \\

Toutefois, l'expression analytique de cette loi a posteriori n'est généralement  pas accessible de façon directe, du fait de la forte non-linéarité des phénomènes pris en compte par le modèle de dispersion. Il est alors nécessaire d'introduire des méthodes numériques d'approximation pour évaluer  $p(\VecSigma | \VecObs)$. En général, il s'agit d'algorithmes de simulation aléatoire appartenant à la famille des méthodes de Monte-Carlo, et qui permettent une exploration optimale de l'espace des solutions.\\

Parmi ces algorithmes, la catégorie la plus connue est certainement celle des méthodes dites \textit{Markov Chain Monte-Carlo} (MCMC). Dans \cite{Keats2007} et \cite{Chow2008}, la méthode MCMC est employée dans un contexte urbain, et couplée à des modèles de dispersion suffisamment performants pour une prise en compte du milieu bâti. \cite{Senocak2008} propose l'utilisation d'un modèle de dispersion plus simple de type gaussien, amélioré par l'ajout de paramètres stochastiques relatifs à la diffusion turbulente, eux-mêmes estimés par l'algorithme MCMC en plus des paramètres de la source. , ce qui permet une exploitation plus efficace des calculs de dispersion. \cite{Yee2008b} étend la méthodologie aux cas où le nombre de sources est inconnu et considéré comme un paramètre supplémentaire à estimer, ajoutant ainsi une étape de sélection de modèle dans la procédure d'estimation. Cela est possible grâce à une méthode MCMC à \textit{sauts réversibles}, qui associe pour chaque nombre de sources possible un espace de paramètres différent à explorer. Enfin, \cite{Yee2014} propose une extension des concepts de \cite{Keats2007} à l'échelle globale, où un algorithme MCMC est utilisé pour reconstruire la position et le profil d'émission d'une usine d'isotopes médicaux à partir des mesures de xénon relevées par le réseau mondial des capteurs de l'OTICE\footnote{L'\textit{Organisation du Traité d'Interdiction Complète des Essais Nucléaires} (OTICE, ou CTBTO en anglais: \textit{Comprehensive Nuclear-Test Ban Treaty Organization)} a pour rôle de détecter, grâce à un réseau de capteurs déployés sur l'ensemble du globe, et de signaler à tous les pays signataires du traité toute explosion d'origine atomique, pour prendre des mesures afin d'empêcher les puissances nucléaires actuelles de poursuivre leurs essais, et les Etats ne disposant pas de l'arme atomique de s'en doter.}. \\
